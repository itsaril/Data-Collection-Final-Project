# ðŸŒ¦ï¸ Weather Data Pipeline: Almaty Real-Time Analytics

This project implements a complete **End-to-End Data Pipeline** that collects, processes, and analyzes high-frequency weather data for Almaty. It combines **Streaming** (via Kafka) and **Batch** (via Airflow) processing to provide daily weather insights.

---

## ðŸ‘¥ Team Information

* **Student 1:** Zhilikbay Arman  #22B
* **Student 2:** Shakhizada Zgansulu #22B030468
* **Student 3:** Myrzakhankyzy Arailym #22B030408

---

## ðŸ—ï¸ System Architecture

The pipeline is divided into three main stages, each orchestrated by a dedicated **Apache Airflow DAG**:

### 1ï¸âƒ£ DAG 1: Continuous Ingestion (Pseudo-Streaming)

* **Source:** [Tomorrow.io API](https://www.tomorrow.io/weather-api/)
* **Interval:** Fetches data every 3 minutes.
* **Target:** Sends raw JSON events to a **Kafka** topic (`raw_weather_events`).

### 2ï¸âƒ£ DAG 2: Cleaning & Storage (Hourly Batch)

* **Source:** Consumes messages from Kafka.
* **Processing:** Uses **Pandas** for data cleaning (handling duplicates, type conversion, and missing values).
* **Target:** Saves cleaned data into **SQLite** (`events` table).

### 3ï¸âƒ£ DAG 3: Daily Analytics (Daily Batch)

* **Source:** Reads from the `events` table.
* **Processing:** Uses **Pandas** to aggregate data (min/max/avg temperatures, total precipitation).
* **Target:** Stores summarized data in the `daily_summary` table.

---

## ðŸ› ï¸ Tech Stack

| Component | Technology |
| --- | --- |
| **Orchestration** | Apache Airflow (LocalExecutor) |
| **Message Broker** | Apache Kafka & Zookeeper |
| **Data Processing** | Python, Pandas |
| **Database** | SQLite & PostgreSQL (for Airflow) |
| **Infrastructure** | Docker & Docker Compose |

---

## ðŸ“‚ Project Structure

```text
project/
â”œâ”€â”€ airflow/
â”‚   â””â”€â”€ dags/                  # Airflow DAG files
â”œâ”€â”€ src/                       # Core Logic
â”‚   â”œâ”€â”€ job1_producer.py       # API -> Kafka logic
â”‚   â”œâ”€â”€ job2_cleaner.py        # Kafka -> SQLite logic (Pandas)
â”‚   â”œâ”€â”€ job3_analytics.py      # Analytics logic (Pandas)
â”‚   â””â”€â”€ db_utils.py            # SQLite helper functions
â”œâ”€â”€ data/                      # Local storage for app.db
â”œâ”€â”€ Dockerfile                 # Custom Airflow image with dependencies
â”œâ”€â”€ docker-compose.yml         # Full infrastructure setup
â””â”€â”€ requirements.txt           # Python libraries

```

---

## ðŸš€ How to Run

1. **Clone the repository:**
```bash
git clone https://github.com/your-username/your-repo-name.git
cd your-repo-name

```
2. **Launch the environment:**
```bash
docker-compose up --build

```


4. **Access Airflow UI:**
Navigate to `http://localhost:8080`
* **Login:** `admin`
* **Password:** `admin`



---

## ðŸ“Š Database Schema

### `events` table (Cleaned Data)

| Column | Type | Description |
| --- | --- | --- |
| `timestamp` | TEXT | Event time |
| `location` | TEXT | Almaty (lat/lon) |
| `temperature` | REAL | Celsius |
| `humidity` | REAL | Percentage |

### `daily_summary` table (Analytics)

| Column | Type | Description |
| --- | --- | --- |
| `date` | TEXT | Summary date |
| `avg_temperature` | REAL | Mean temperature |
| `record_count` | INT | Total samples per day |
